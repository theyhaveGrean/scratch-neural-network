{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d0799e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67e342e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        self.weights = []\n",
    "        for _ in range(input_size):\n",
    "            self.weights.append(random.uniform(-1, 1))\n",
    "        \n",
    "        self.bias = random.uniform(-1, 1)\n",
    "    \n",
    "    def output(self, inputs):\n",
    "        return max(0, np.dot(self.weights, inputs) + self.bias)\n",
    "        \n",
    "    \n",
    "    def nudge(self, amt_weight, amt_bias):\n",
    "        new_weights = []\n",
    "        for weight in self.weights:\n",
    "            new_weights.append(weight + amt_weight)\n",
    "        self.weights = new_weights\n",
    "        \n",
    "        new_bias = self.bias + amt_bias\n",
    "        self.bias = new_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "910bb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, size, input_size, name):\n",
    "        self.size = size\n",
    "        self.input_size = input_size\n",
    "        self.name = name\n",
    "        self.neurons = []\n",
    "        \n",
    "        print(f'Layer {name} | IS {self.input_size}')\n",
    "        for i in range(self.size):\n",
    "            self.neurons.append(Neuron(input_size=self.input_size))\n",
    "            print(f'Neuron {i + 1} | B {len(self.neurons[i].weights)} | B {self.neurons[i].bias}')\n",
    "        print('==========================================')\n",
    "\n",
    "    def output(self, inputs):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output(inputs))\n",
    "        return outputs\n",
    "    \n",
    "    def nudge(self, amt_weight, amt_bias):\n",
    "        for neuron in self.neurons:\n",
    "            neuron.nudge(amt_weight, amt_bias)\n",
    "    \n",
    "    def forward(self, layer):\n",
    "        outputs = []\n",
    "        for neuron in self.neurons:\n",
    "            outputs.append(neuron.output(self.inputs))\n",
    "        layer.inputs = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "024a754b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(output, desired_output):\n",
    "    return (output - desired_output) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9157e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_sensitivity(desired_output, final_output, front_weight, back_activation, layer_weight, layer_bias):\n",
    "    unactivated = (layer_weight * back_activation) + layer_bias\n",
    "    return 0 if unactivated < 0 else (-2 * (front_weight * back_activation) * (final_output - desired_output))\n",
    "\n",
    "def bias_sensitivity(desired_output, final_output, front_weight):\n",
    "    return (-2 * front_weight * (final_output - desired_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecad2c2",
   "metadata": {},
   "source": [
    "desired_output = 5\n",
    "for epoch in range(1000):\n",
    "    for layer in all layers but the last:\n",
    "        forward\n",
    "    \n",
    "    current_output = last layer's final output\n",
    "    for layer in reversed list of layers WITH INDEX:\n",
    "        variables:\n",
    "            w = weight of layer in front of layer (1 if output layer)\n",
    "            a = activation of layer behind current layer\n",
    "        \n",
    "        weight sensitivity = function(output, weight of front layer, activation of back layer, desired output)\n",
    "        bias sensitivity = function(desired output, weight of front layer, output)\n",
    "        \n",
    "        nudge neuron in layer by sensitivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "64eb5b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer INPUT | IS 1\n",
      "Neuron 1 | B 1 | B 0.12860185634195398\n",
      "==========================================\n",
      "Layer OUTPUT | IS 1\n",
      "Neuron 1 | B 1 | B -0.56153853558588\n",
      "==========================================\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers = [    \n",
    "    Layer(size=1,\n",
    "         input_size=1,\n",
    "         name='INPUT'),\n",
    "        Layer(size=1,\n",
    "         input_size=1,\n",
    "         name='OUTPUT')\n",
    "]\n",
    "\n",
    "layers[0].inputs = [1]\n",
    "\n",
    "desired_output = 5\n",
    "\n",
    "inputs = []\n",
    "outputs = []\n",
    "for epoch in range(1000):\n",
    "    for i in range(len(layers) - 1):\n",
    "        layers[i].forward(layers[i + 1])\n",
    "    \n",
    "    final_output = layers[-1].output(layers[-1].inputs)[0]\n",
    "    \n",
    "    for layer in layers:\n",
    "        index = layers.index(layer)\n",
    "        weight_front = 1 if index == len(layers) - 1 else layers[index + 1].neurons[0].weights[0]\n",
    "        activation_back = layer.inputs[0] # previous activation becomes inputs, right?\n",
    "        layer_weight = layer.neurons[0].weights[0]\n",
    "        layer_bias = layer.neurons[0].bias\n",
    "        \n",
    "        w_sens = weight_sensitivity(desired_output=desired_output,\n",
    "                                   final_output=final_output,\n",
    "                                    front_weight=weight_front,\n",
    "                                    back_activation=activation_back,\n",
    "                                    layer_weight=layer_weight,\n",
    "                                    layer_bias=layer_bias)\n",
    "        \n",
    "        b_sens = bias_sensitivity(desired_output=desired_output,\n",
    "                                 final_output=final_output,\n",
    "                                 front_weight=weight_front)\n",
    "        \n",
    "        layer.neurons[0].nudge(w_sens * 0.01, b_sens * 0.01)\n",
    "        \n",
    "    inputs.append(epoch)\n",
    "    outputs.append(final_output)\n",
    "    \n",
    "plt.plot(inputs, outputs, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a74907ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer INPUT | IS 1\n",
      "Neuron 1 | B 1 | B -0.8690269018844627\n",
      "==========================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "weight_sensitivity() missing 1 required positional argument: 'layer_bias'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m out \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mneurons[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39moutput([inputs])\n\u001b[0;32m     26\u001b[0m c \u001b[38;5;241m=\u001b[39m cost(out, desired_out)\n\u001b[1;32m---> 27\u001b[0m w_sens \u001b[38;5;241m=\u001b[39m \u001b[43mweight_sensitivity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesired_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m b_sens \u001b[38;5;241m=\u001b[39m bias_sensitivity(out, desired_out, inputs)\n\u001b[0;32m     29\u001b[0m o_sens \u001b[38;5;241m=\u001b[39m prev_output_sensitivity(out, desired_out, inputs, weight)\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_sensitivity() missing 1 required positional argument: 'layer_bias'"
     ]
    }
   ],
   "source": [
    "layers = [    \n",
    "    Layer(size=1,\n",
    "         input_size=1,\n",
    "         name='INPUT')\n",
    "]\n",
    "layers[0].inputs=[1]\n",
    "\n",
    "inps=[]\n",
    "outputs=[]\n",
    "  \n",
    "\n",
    "for x in range(10000):  \n",
    "    for i in range(len(layers) - 1):\n",
    "        layers[i].forward(layers[i + 1])\n",
    "        \n",
    "    for layer in reversed(layers):\n",
    "        if layers.index(layer) == len(layers) - 1:\n",
    "            desired_out = 2\n",
    "        else:\n",
    "            desired_out = (layer.output(layer.inputs)[0] - o_sens) * .01\n",
    "        inputs = layer.inputs[0]\n",
    "        weight = layer.neurons[0].weights[0]\n",
    "        bias = layer.neurons[0].bias\n",
    "        out = layer.neurons[0].output([inputs])\n",
    "        \n",
    "        c = cost(out, desired_out)\n",
    "        w_sens = weight_sensitivity(out, desired_out, weight, inputs, bias)\n",
    "        b_sens = bias_sensitivity(out, desired_out, inputs)\n",
    "        o_sens = prev_output_sensitivity(out, desired_out, inputs, weight)\n",
    "        \n",
    "        layer.nudge(w_sens * .01, b_sens * .01 )\n",
    "    \n",
    "        print(f'Output {out}')\n",
    "        print(f'Cost {c}')\n",
    "        print(f'Weight Sensitivity {w_sens}')\n",
    "        print(f'Bias Sensitivity {b_sens}')\n",
    "        print(f'Previous Output Sensitivity {o_sens}')\n",
    "        print('====================================')\n",
    "    \n",
    "    \n",
    "    \n",
    "    inps.append(x)\n",
    "    outputs.append(layers[-1].output(layers[-1].inputs))    \n",
    "plt.title('Output per Epoch')\n",
    "plt.plot(inps, outputs, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8611ec25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "layers = [    \n",
    "    Layer(size=1,\n",
    "         input_size=1,\n",
    "         name='INPUT'),\n",
    "    Layer(size=1,\n",
    "         input_size=1,\n",
    "         name='HIDDEN1'),\n",
    "    Layer(size=1,\n",
    "         input_size=1,\n",
    "         name='OUTPUT')\n",
    "]\n",
    "layers[0].inputs=[1]\n",
    "\n",
    "inps=[]\n",
    "outputs=[]\n",
    "  \n",
    "for x in range(1000):  \n",
    "    for i in range(len(layers) - 1):\n",
    "        layers[i].forward(layers[i + 1])\n",
    "        \n",
    "    for layer in reversed(layers):\n",
    "        if layers.index(layer) == len(layers) - 1:\n",
    "            desired_out = 5\n",
    "        else:\n",
    "            desired_out = (layer.output(layer.inputs)[0] - o_sens) * .001\n",
    "        inputs = layer.inputs[0]\n",
    "        weight = layer.neurons[0].weights[0]\n",
    "        bias = layer.neurons[0].bias\n",
    "        out = layer.neurons[0].output([inputs])\n",
    "        \n",
    "        c = cost(out, desired_out)\n",
    "        w_sens = weight_sensitivity(out, desired_out, weight, inputs, bias)\n",
    "        b_sens = bias_sensitivity(out, desired_out, inputs)\n",
    "        o_sens = prev_output_sensitivity(out, desired_out, inputs, weight)\n",
    "        \n",
    "        layer.nudge(w_sens * .01, b_sens * .01 )\n",
    "    \n",
    "        print(f'{layers.index(layer)}Output {out}')\n",
    "        print(f'Cost {c}')\n",
    "        print(f'Weight Sensitivity {w_sens}')\n",
    "        print(f'Bias Sensitivity {b_sens}')\n",
    "        print(f'Previous Output Sensitivity {o_sens}')\n",
    "        print('====================================')\n",
    "    \n",
    "    \n",
    "    \n",
    "    inps.append(x)\n",
    "    outputs.append(layers[-1].output(layers[-1].inputs))    \n",
    "plt.title('Output per Epoch')\n",
    "plt.plot(inps, outputs, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f806543",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers[0].inputs = [9]\n",
    "layers[0].forward(layers[1])\n",
    "layers[1].forward(layers[2])\n",
    "print(layers[2].output(layers[2].inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
